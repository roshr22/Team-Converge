{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# FF++ Staged Fine-tuning (Robust Version)\n",
                "\n",
                "**Data**: 17GB FF++ dataset (7011 files)\n",
                "\n",
                "**Stages:**\n",
                "- **A**: Head-only stabilization (2 epochs)\n",
                "- **B**: Partial unfreeze - layer4 (8 epochs)\n",
                "- **C**: Optional deeper unfreeze (5 epochs)\n",
                "\n",
                "**Estimated Time**: ~2-3 hours total on T4 GPU"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Environment Check & Setup\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Check GPU first - fail fast if not available\n",
                "import torch\n",
                "if not torch.cuda.is_available():\n",
                "    print(\"ERROR: No GPU detected!\")\n",
                "    print(\"Go to: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
                "    raise SystemExit(\"GPU required\")\n",
                "\n",
                "gpu_name = torch.cuda.get_device_name(0)\n",
                "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.version.cuda}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. Mount Drive & Clone Repo\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Clone fresh repo\n",
                "!rm -rf /content/Team-Converge\n",
                "!git clone https://github.com/Incharajayaram/Team-Converge.git /content/Team-Converge\n",
                "%cd /content/Team-Converge/Finetune1\n",
                "\n",
                "# Verify we're in the right place\n",
                "assert os.path.exists('config.yaml'), \"ERROR: config.yaml not found!\"\n",
                "assert os.path.exists('train_staged.py'), \"ERROR: train_staged.py not found!\"\n",
                "print(\"Repo cloned successfully!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. Install Dependencies\n",
                "!pip install -q mediapipe pyyaml tqdm gdown scikit-learn\n",
                "\n",
                "# Verify critical imports\n",
                "try:\n",
                "    import mediapipe\n",
                "    import yaml\n",
                "    import gdown\n",
                "    from sklearn.metrics import roc_auc_score\n",
                "    print(\"All dependencies installed!\")\n",
                "except ImportError as e:\n",
                "    print(f\"ERROR: Missing dependency - {e}\")\n",
                "    raise"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. Download FF++ Data from Drive (17GB - takes 5-10 min)\n",
                "import os\n",
                "import subprocess\n",
                "\n",
                "# Updated FILE_ID - Jan 2026\n",
                "FILE_ID = \"1a7X9Cjv3gsj4qC6kcDq6VLNl7eR3osoy\"\n",
                "ZIP_PATH = \"/content/ffpp_data.zip\"\n",
                "EXPECTED_SIZE_GB = 17.0\n",
                "\n",
                "# Remove any previous partial/corrupted download\n",
                "if os.path.exists(ZIP_PATH):\n",
                "    print(f\"Removing existing {ZIP_PATH}...\")\n",
                "    os.remove(ZIP_PATH)\n",
                "\n",
                "# Upgrade gdown for large file support\n",
                "!pip install -q --upgrade gdown\n",
                "\n",
                "print(f\"Downloading {EXPECTED_SIZE_GB}GB file from Google Drive...\")\n",
                "print(\"This will take 5-10 minutes depending on connection speed.\")\n",
                "\n",
                "# Download with explicit ID flag for large files\n",
                "!gdown --id {FILE_ID} --output {ZIP_PATH} --fuzzy\n",
                "\n",
                "# Verify download\n",
                "if not os.path.exists(ZIP_PATH):\n",
                "    print(\"\\nERROR: Download failed! File not found.\")\n",
                "    print(\"\\nTry manual method:\")\n",
                "    print(\"1. Upload ffpp_data_new.zip to your Google Drive\")\n",
                "    print(\"2. Run: !cp '/content/drive/MyDrive/ffpp_data_new.zip' /content/ffpp_data.zip\")\n",
                "    raise FileNotFoundError(\"Download failed\")\n",
                "\n",
                "size_gb = os.path.getsize(ZIP_PATH) / 1e9\n",
                "print(f\"\\nDownloaded: {size_gb:.2f} GB\")\n",
                "\n",
                "if size_gb < 15:\n",
                "    print(\"\\nWARNING: File too small! Download may have failed.\")\n",
                "    print(\"The file might be an HTML error page.\")\n",
                "    print(\"\\nCheck file content:\")\n",
                "    !head -c 200 {ZIP_PATH}\n",
                "    print(\"\\n\\nTry manual method:\")\n",
                "    print(\"!cp '/content/drive/MyDrive/ffpp_data_new.zip' /content/ffpp_data.zip\")\n",
                "    raise ValueError(f\"File too small: {size_gb:.2f} GB\")\n",
                "else:\n",
                "    print(\"Download OK!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 5. Extract Data (takes 2-3 min for 17GB)\n",
                "import zipfile\n",
                "\n",
                "EXTRACT_PATH = \"/content/data/raw/ffpp\"\n",
                "ZIP_PATH = \"/content/ffpp_data.zip\"\n",
                "\n",
                "# Clean previous extraction\n",
                "!rm -rf {EXTRACT_PATH}\n",
                "!mkdir -p {EXTRACT_PATH}\n",
                "\n",
                "print(\"Extracting (this takes 2-3 minutes)...\")\n",
                "\n",
                "# Test zip integrity first\n",
                "try:\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
                "        # Quick test - just read the file list\n",
                "        file_count = len(zf.namelist())\n",
                "        print(f\"Zip contains {file_count} files\")\n",
                "except zipfile.BadZipFile:\n",
                "    print(\"\\nERROR: Zip file is corrupted!\")\n",
                "    print(\"The download was incomplete or corrupted.\")\n",
                "    print(\"\\nSolutions:\")\n",
                "    print(\"1. Re-run the download cell\")\n",
                "    print(\"2. Or manually copy: !cp '/content/drive/MyDrive/ffpp_data_new.zip' /content/ffpp_data.zip\")\n",
                "    raise\n",
                "\n",
                "# Extract using unzip (faster than Python for large files)\n",
                "!unzip -q {ZIP_PATH} -d {EXTRACT_PATH}\n",
                "\n",
                "# Verify extraction\n",
                "items = os.listdir(EXTRACT_PATH)\n",
                "print(f\"\\nExtracted {len(items)} top-level items: {items}\")\n",
                "\n",
                "# Find the FF++ data folder\n",
                "ffpp_folder = None\n",
                "for item in items:\n",
                "    if 'FaceForensics' in item or 'ffpp' in item.lower():\n",
                "        ffpp_folder = os.path.join(EXTRACT_PATH, item)\n",
                "        break\n",
                "\n",
                "if ffpp_folder:\n",
                "    print(f\"FF++ data found at: {ffpp_folder}\")\n",
                "    # Update EXTRACT_PATH for training\n",
                "    FFPP_ROOT = ffpp_folder\n",
                "else:\n",
                "    FFPP_ROOT = EXTRACT_PATH\n",
                "    \n",
                "# Count total videos\n",
                "total_videos = 0\n",
                "for root, dirs, files in os.walk(FFPP_ROOT):\n",
                "    total_videos += sum(1 for f in files if f.endswith('.mp4'))\n",
                "print(f\"Total video files: {total_videos}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 6. Verify Data Structure\n",
                "import os\n",
                "\n",
                "# Auto-detect the correct ffpp root\n",
                "possible_roots = [\n",
                "    \"/content/data/raw/ffpp\",\n",
                "    \"/content/data/raw/ffpp/FaceForensics++_C23\",\n",
                "    \"/content/data/raw/ffpp/ffpp_data\"\n",
                "]\n",
                "\n",
                "FFPP_ROOT = None\n",
                "for root in possible_roots:\n",
                "    # Check if this folder has the expected structure\n",
                "    if os.path.exists(root):\n",
                "        subdirs = os.listdir(root) if os.path.isdir(root) else []\n",
                "        # Look for manipulation method folders or video files\n",
                "        has_videos = any(f.endswith('.mp4') for f in subdirs)\n",
                "        has_method_folders = any(d in subdirs for d in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures', 'original', 'DeepFakeDetection'])\n",
                "        if has_videos or has_method_folders or 'csv' in subdirs:\n",
                "            FFPP_ROOT = root\n",
                "            break\n",
                "\n",
                "if FFPP_ROOT is None:\n",
                "    print(\"WARNING: Could not auto-detect FF++ root. Using default.\")\n",
                "    FFPP_ROOT = \"/content/data/raw/ffpp\"\n",
                "    \n",
                "print(f\"Using FFPP_ROOT: {FFPP_ROOT}\")\n",
                "print(f\"Contents: {os.listdir(FFPP_ROOT)[:10]}...\")\n",
                "\n",
                "# Save for later cells\n",
                "os.environ['FFPP_ROOT'] = FFPP_ROOT"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 7. Create output directory on Drive\n",
                "OUTPUT_DIR = \"/content/drive/MyDrive/ffpp_training/staged\"\n",
                "!mkdir -p {OUTPUT_DIR}\n",
                "\n",
                "# Also create local cache dir\n",
                "CACHE_DIR = \"/content/cache/faces\"\n",
                "!mkdir -p {CACHE_DIR}\n",
                "\n",
                "print(f\"Output will be saved to: {OUTPUT_DIR}\")\n",
                "print(f\"Face cache: {CACHE_DIR}\")\n",
                "\n",
                "# Check available disk space\n",
                "!df -h /content | tail -1"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 8. Run Stage A: Head-only stabilization (2 epochs)\n",
                "import os\n",
                "FFPP_ROOT = os.environ.get('FFPP_ROOT', '/content/data/raw/ffpp')\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STAGE A: Head-only training (2 epochs)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "!python train_staged.py --config config.yaml \\\n",
                "    --override dataset.ffpp_root={FFPP_ROOT} \\\n",
                "    --override caching.cache_dir=/content/cache/faces \\\n",
                "    --stages A \\\n",
                "    --output_dir /content/drive/MyDrive/ffpp_training/staged"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 9. Run Stage B: Partial unfreeze - layer4 (8 epochs)\n",
                "import os\n",
                "FFPP_ROOT = os.environ.get('FFPP_ROOT', '/content/data/raw/ffpp')\n",
                "\n",
                "# Check if Stage A checkpoint exists\n",
                "checkpoint = \"/content/drive/MyDrive/ffpp_training/staged/best_model.pt\"\n",
                "if not os.path.exists(checkpoint):\n",
                "    print(f\"ERROR: Checkpoint not found at {checkpoint}\")\n",
                "    print(\"Make sure Stage A completed successfully!\")\n",
                "    raise FileNotFoundError(checkpoint)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STAGE B: Partial unfreeze - layer4 (8 epochs)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "!python train_staged.py --config config.yaml \\\n",
                "    --override dataset.ffpp_root={FFPP_ROOT} \\\n",
                "    --override caching.cache_dir=/content/cache/faces \\\n",
                "    --stages B \\\n",
                "    --resume {checkpoint} \\\n",
                "    --output_dir /content/drive/MyDrive/ffpp_training/staged"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 10. (Optional) Stage C: Deeper unfreeze - run only if Stage B plateaus\n",
                "# Uncomment the lines below to run Stage C\n",
                "\n",
                "# import os\n",
                "# FFPP_ROOT = os.environ.get('FFPP_ROOT', '/content/data/raw/ffpp')\n",
                "# \n",
                "# print(\"=\"*60)\n",
                "# print(\"STAGE C: Deeper unfreeze (5 epochs)\")\n",
                "# print(\"=\"*60)\n",
                "# \n",
                "# !python train_staged.py --config config.yaml \\\n",
                "#     --override dataset.ffpp_root={FFPP_ROOT} \\\n",
                "#     --override caching.cache_dir=/content/cache/faces \\\n",
                "#     --stages C \\\n",
                "#     --resume /content/drive/MyDrive/ffpp_training/staged/best_model.pt \\\n",
                "#     --output_dir /content/drive/MyDrive/ffpp_training/staged"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 11. View Training History\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "\n",
                "history_path = '/content/drive/MyDrive/ffpp_training/staged/training_history.json'\n",
                "\n",
                "if not os.path.exists(history_path):\n",
                "    print(f\"Training history not found at {history_path}\")\n",
                "    print(\"Training may not have completed yet.\")\n",
                "else:\n",
                "    with open(history_path) as f:\n",
                "        history = json.load(f)\n",
                "\n",
                "    epochs = [h['epoch'] for h in history]\n",
                "    train_loss = [h['train_loss'] for h in history]\n",
                "    val_loss = [h['val_loss'] for h in history]\n",
                "    val_auc = [h.get('val_auc', 0.5) for h in history]\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "    ax1.plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2)\n",
                "    ax1.plot(epochs, val_loss, 'r-', label='Val Loss', linewidth=2)\n",
                "    ax1.set_xlabel('Epoch')\n",
                "    ax1.set_ylabel('Loss')\n",
                "    ax1.legend()\n",
                "    ax1.set_title('Loss Curves')\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "\n",
                "    ax2.plot(epochs, val_auc, 'g-', label='Val AUC', linewidth=2)\n",
                "    ax2.set_xlabel('Epoch')\n",
                "    ax2.set_ylabel('AUC')\n",
                "    ax2.set_ylim(0.5, 1.0)\n",
                "    ax2.legend()\n",
                "    ax2.set_title('Validation AUC')\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('/content/drive/MyDrive/ffpp_training/training_curves.png', dpi=150)\n",
                "    plt.show()\n",
                "\n",
                "    print(f\"\\nBest val_loss: {min(val_loss):.4f} (epoch {epochs[val_loss.index(min(val_loss))]})\")\n",
                "    print(f\"Best val_auc: {max(val_auc):.4f} (epoch {epochs[val_auc.index(max(val_auc))]})\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 12. Final Summary & Save\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "output_dir = '/content/drive/MyDrive/ffpp_training/staged'\n",
                "final_model = '/content/drive/MyDrive/ffpp_training/final_model.pt'\n",
                "\n",
                "# Copy best model to final location\n",
                "best_model = os.path.join(output_dir, 'best_model.pt')\n",
                "if os.path.exists(best_model):\n",
                "    shutil.copy(best_model, final_model)\n",
                "    size_mb = os.path.getsize(final_model) / 1e6\n",
                "    print(f\"Final model saved: {final_model} ({size_mb:.1f} MB)\")\n",
                "else:\n",
                "    print(f\"WARNING: Best model not found at {best_model}\")\n",
                "\n",
                "# List all output files\n",
                "print(\"\\nOutput files:\")\n",
                "for f in os.listdir(output_dir):\n",
                "    path = os.path.join(output_dir, f)\n",
                "    size = os.path.getsize(path) / 1e6 if os.path.isfile(path) else 0\n",
                "    print(f\"  {f}: {size:.1f} MB\" if size > 0 else f\"  {f}/\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}