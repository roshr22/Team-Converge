# Training hyperparameters

training:
  epochs: 50
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 1e-4

  # Optimizer
  optimizer: "adam"
  momentum: 0.9

  # Learning rate schedule
  scheduler: "cosine"
  warmup_epochs: 5
  min_lr: 1e-6

  # Checkpointing
  save_interval: 5  # epochs
  save_best: true

  # Logging
  log_interval: 100  # iterations
  val_interval: 5  # epochs

  # Device
  device: "cuda"
  mixed_precision: false

  # Seed
  seed: 42

# Knowledge Distillation Configuration
# CRITICAL: alpha values optimized to fix AUC=0.5 saturation issue
# Low alpha_distill (0.1) lets task loss dominate and learn from labels
# Saturation guard in distillation.py handles extreme pretrained logits
distillation:
  temperature: 1.0  # No temperature scaling (logits normalized by saturation guard)
  alpha_distill: 0.1  # Low weight: prevent pretrained teacher from dominating
  alpha_task: 0.9  # High weight: ensure student learns task from labels
  use_attention_transfer: false

